{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c494d9",
   "metadata": {},
   "source": [
    "This is submission code for the Kaggle competition\n",
    "\n",
    "Playground Series - Season 3, Episode 5\n",
    "\n",
    "https://www.kaggle.com/competitions/playground-series-s3e5\n",
    "\n",
    "which came in position 153 of 903, which is okay but worse than the previous competition result, despite this notebook being more sophisticated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff9ef8",
   "metadata": {},
   "source": [
    "# Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8409b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Scikit stuff\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# XGB\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# TPOT\n",
    "from tpot import TPOTRegressor, TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be0c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/medion/playground-series-s3e5/train.csv')\n",
    "eval_data = pd.read_csv('C:/Users/medion/playground-series-s3e5/test.csv')\n",
    "sample_submit = pd.read_csv('C:/Users/medion/playground-series-s3e5/sample_submission.csv')\n",
    "\n",
    "data = data.drop(\"Id\", axis=1)\n",
    "eval_data = eval_data.drop('Id', axis=1)\n",
    "\n",
    "target_col = 'quality'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe736696",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "We have classes for the train and test data. It contains possible functions we can use for data processing. Some are used for data exploration so are not used in the final notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    def __init__(self, data):\n",
    "        self.data_original = data\n",
    "        self.data = self.data_original.copy()\n",
    "        \n",
    "    def histplot(self):\n",
    "        '''\n",
    "        Plots a histogram for each feature, this might be called after transforming features,\n",
    "        use original_data_histplot() to call this with the original data\n",
    "        '''\n",
    "        for column in self.data.columns:\n",
    "            sns.histplot(data = self.data[column])\n",
    "            plt.show()\n",
    "            \n",
    "    def original_data_histplot(self):\n",
    "        '''\n",
    "        Plots a histogram for each feature in the original data\n",
    "        '''\n",
    "        for column in self.data_original.columns:\n",
    "            sns.histplot(data = self.data_original[column])\n",
    "            plt.show()\n",
    "    \n",
    "    def drop(self, col_list):\n",
    "        '''\n",
    "        Drops a list of features\n",
    "        '''\n",
    "        for col in col_list:\n",
    "            self.data = self.data.drop(col, axis=1)\n",
    "    \n",
    "    def value_counts(self):\n",
    "        '''\n",
    "        Returns data counts for each feature\n",
    "        '''\n",
    "        for column in self.data.columns:\n",
    "            print(f\"{column}\")\n",
    "            print(self.data[column].value_counts())\n",
    "            print(\"-------------------------------------\")\n",
    "        \n",
    "    def remove_outliers(self):\n",
    "        '''\n",
    "        Removes outliers from the dataset. These were chosen by inspection. Here we have certain target values\n",
    "        only appearing a few times, so we should be careful not to remove data with one of those target values.\n",
    "        '''\n",
    "        self.data = self.data[self.data['residual sugar'] < 10]\n",
    "        self.data = self.data[self.data['chlorides'] < 0.3]\n",
    "        self.data = self.data[self.data['chlorides'] > 0.02]\n",
    "        self.data = self.data[self.data['free sulfur dioxide'] < 60]\n",
    "        self.data = self.data[self.data['total sulfur dioxide'] < 160]\n",
    "        self.data = self.data[self.data['sulphates'] < 1.9]\n",
    "        self.data = self.data[self.data['pH'] > 2.8]\n",
    "        self.data = self.data[self.data['volatile acidity'] < 1.5]\n",
    "        self.data = self.data[self.data['residual sugar'] > 1.3]\n",
    "        \n",
    "        self.data = self.data.reset_index()\n",
    "        self.data = self.data.drop('index', axis = 1)\n",
    "\n",
    "    def show_correlation(self):\n",
    "        '''\n",
    "        Show n highest and lowest correlated variables for each variable\n",
    "        '''\n",
    "        corr = pd.DataFrame(self.data.corr())\n",
    "        for column in self.data.columns:\n",
    "            print(f'Three lowest correlation for {column}')\n",
    "            print(pd.DataFrame(corr[column]).sort_values(column).iloc[0:12])\n",
    "            print('----------------------------------------------------')\n",
    "            print(f'Three highest correlation for {column}')\n",
    "            print(pd.DataFrame(corr[column]).sort_values(column).iloc[-12-1:-1])\n",
    "            print('-----------------------------------------------------')\n",
    "            \n",
    "    def power_transform(self):\n",
    "        target_list = [target_col]\n",
    "        power_data = [item for item in self.data.columns if item not in target_list]\n",
    "        \n",
    "        for item in power_data:\n",
    "            pt = PowerTransformer()\n",
    "            self.data[item+' *'] = pt.fit_transform(pd.DataFrame(self.data[item]))\n",
    "            self.data = self.data.drop(item, axis=1)\n",
    "\n",
    "    def add_features(self): \n",
    "        '''\n",
    "        Add features to the data set, this was done by adding using the below functions to add all\n",
    "        products, ratios, reciprocals, roots, logs, and exps and seeing which were useful. We also drop pH,\n",
    "        which gave better results.\n",
    "        '''\n",
    "        self.data['alcohol * pH'] = self.data['alcohol'] * self.data['pH']\n",
    "        self.data['density / sulphates'] = self.data['density'] / self.data['sulphates']\n",
    "        self.data['sulphates / total sulfur dioxide'] = self.data['sulphates'] / self.data['total sulfur dioxide']\n",
    "        self.data['density / chlorides'] = self.data['density'] / self.data['chlorides']\n",
    "        self.data['total sulfur dioxide / density'] = self.data['total sulfur dioxide'] / self.data['density']\n",
    "        self.data['citric acid * pH'] = self.data['citric acid'] * self.data['pH']\n",
    "        self.data['total sulfur dioxide / volatile acidity'] = self.data['total sulfur dioxide'] / self.data['volatile acidity']\n",
    "        self.data['density / fixed acidity'] = self.data['density'] / self.data['fixed acidity']\n",
    "        self.data['sulphates * fixed acidity'] = self.data['sulphates'] * self.data['fixed acidity']\n",
    "        self.data['density * chlorides'] = self.data['density'] * self.data['chlorides']\n",
    "        self.data['1/free sulfur dioxide'] = 1/self.data['free sulfur dioxide']\n",
    "        self.data['log sulphates'] = np.log(self.data['sulphates'])\n",
    "        self.data['root sulphates'] = np.sqrt(self.data['sulphates'])\n",
    "        self.data['log pH'] = np.log(self.data['pH'])\n",
    "        self.data['root pH'] = np.sqrt(self.data['pH'])\n",
    "        self.data['exp pH'] = np.exp(self.data['pH'])\n",
    "        self.data['1/alcohol'] = 1/self.data['alcohol']\n",
    "        self.data['1/density'] = 1/self.data['density']\n",
    "        self.data['1/chlorides'] = 1/self.data['chlorides']\n",
    "        self.data['1/residual sugar'] = 1/self.data['residual sugar']\n",
    "        self.data['1/pH'] = 1/self.data['pH']\n",
    "        self.data['1/fixed acidity'] = 1/self.data['fixed acidity']\n",
    "        self.data['1/total sulfur dioxide'] = 1/self.data['total sulfur dioxide'] \n",
    "        \n",
    "        self.data = self.data.drop('pH', axis=1)\n",
    "        \n",
    "    def add_all_products(self):\n",
    "        '''\n",
    "        Adds features corresponding to the products of each pair of features\n",
    "        '''\n",
    "        pairs = []\n",
    "        names = []\n",
    "        for column in self.data_original.columns:\n",
    "            for column_pair in self.data_original.columns:\n",
    "                if column != target_col and column_pair != target_col:\n",
    "                    pairs.append(self.data[column] * self.data[column_pair])\n",
    "                    names.append(f'{column} * {column_pair}')\n",
    "        new = pd.DataFrame(pairs).transpose()\n",
    "        new.columns = names\n",
    "        self.data = pd.concat([self.data, new], axis=1)\n",
    "        \n",
    "    def add_all_ratios(self):\n",
    "        '''\n",
    "        Adds features corresponding to the ratios of each pair of features (provided we don't divide by 0)\n",
    "        '''\n",
    "        pairs = []\n",
    "        names = []\n",
    "        for column in self.data_original.columns:\n",
    "            for column_pair in self.data_original.columns:\n",
    "                if column_pair != column and column != target_col and column_pair != target_col \\\n",
    "                and (self.data[column] / self.data[column_pair]).sum() != np.inf:\n",
    "                    pairs.append(self.data[column] / self.data[column_pair])\n",
    "                    names.append(f'{column} / {column_pair}')\n",
    "        new = pd.DataFrame(pairs).transpose()\n",
    "        new.columns = names\n",
    "        self.data = pd.concat([self.data, new], axis=1)\n",
    "        \n",
    "    def add_all_reciprocals(self):\n",
    "        '''\n",
    "        Adds features corresponding to the reciprocals of each feature (provided we don't divide by 0)\n",
    "        '''\n",
    "        reciprocal_cols = []\n",
    "        names = []\n",
    "        for column in self.data_original.columns:\n",
    "            if column != target_col and (1/self.data[column]).sum() != np.inf:\n",
    "                reciprocal_cols.append(1/self.data[column])\n",
    "                names.append(f'1/{column}')\n",
    "        new = pd.DataFrame(reciprocal_cols).transpose()\n",
    "        new.columns = names\n",
    "        self.data = pd.concat([self.data, new], axis=1)\n",
    "        \n",
    "    def add_all_roots(self):\n",
    "        '''\n",
    "        Adds features corresponding to the square roots of each feature\n",
    "        '''\n",
    "        for column in self.data_original.columns:\n",
    "            if column != target_col and (self.data[column] >= 0).all():\n",
    "                self.data[f'root {column}'] = np.sqrt(self.data[column])\n",
    "                \n",
    "    def add_all_logs(self):\n",
    "        '''\n",
    "        Adds features corresponding to the log of each feature (provided features are >0)\n",
    "        '''\n",
    "        for column in self.data_original.columns:\n",
    "            if column != target_col and (self.data[column] > 0).all():\n",
    "                self.data[f'log {column}'] = np.log(self.data[column])\n",
    "\n",
    "    def add_all_exp(self):\n",
    "        '''\n",
    "        Adds features corresponding to the exp of each feature\n",
    "        '''\n",
    "        for column in self.data_original.columns:\n",
    "            if column != target_col:\n",
    "                self.data[f'exp {column}'] = np.exp(self.data[column])\n",
    "        \n",
    "    def general_add_all_features(self):\n",
    "        '''\n",
    "        Adds all features from above, allowing us to see which are useful.\n",
    "        '''\n",
    "        self.add_all_roots()\n",
    "        self.add_all_logs()\n",
    "        self.add_all_exp()\n",
    "        self.add_all_reciprocals()\n",
    "        self.add_all_products()\n",
    "        self.add_all_ratios()\n",
    "        return\n",
    "        \n",
    "    def pipeline(self):\n",
    "        '''\n",
    "        Pipeline of chosen data processing methods\n",
    "        '''\n",
    "        self.remove_outliers()\n",
    "        self.add_features()\n",
    "        self.power_transform()  \n",
    "        \n",
    "# Process data and split features, target     \n",
    "\n",
    "train_class = DataProcess(data)\n",
    "train_class.pipeline()\n",
    "X_train_origin = train_class.data.drop(target_col, axis=1)\n",
    "y_train = train_class.data[target_col]\n",
    "\n",
    "# We need to label encode since y_train values are 3,4,5,6,7,8, which XGBoost doesn't like\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = [target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e376c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalProcess(DataProcess):\n",
    "    def __init__(self, data):\n",
    "        self.data_original = data\n",
    "        self.data = self.data_original.copy()\n",
    "    \n",
    "    def cols(self):\n",
    "        '''\n",
    "        Run this to ensure train and eval data have the same columns\n",
    "        '''\n",
    "        eval_cols = [\n",
    "            column for column in self.data.columns if column in train_class.data.columns\n",
    "        ]\n",
    "        self.data = self.data[eval_cols]\n",
    "    \n",
    "    def pipeline(self):\n",
    "        self.add_features()\n",
    "        self.power_transform()\n",
    "        self.cols()\n",
    "\n",
    "# Process eval data        \n",
    "        \n",
    "eval_class = EvalProcess(eval_data)\n",
    "eval_class.pipeline()\n",
    "\n",
    "eval_data_origin = eval_class.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fa1ae",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "We search over a selection of parameters to find the best XGB model. This is a much tidier version of the code from the previous playground series competition.\n",
    "\n",
    "Since we have a number of target values that only appear a few times, we use skikit's resample to randomly choose the number of target values that appear. We also randomly choose a selection of feature columns.\n",
    "\n",
    "This has a feature that, if we see a poor kappa result on one of the K-fold slices then we will just skip to the next iteration, saving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547ea985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 36 kappa: 0.5284889955395059\n",
      "Step: 68 kappa: 0.5371871035279868\n",
      "Step: 116 kappa: 0.5358582275317664\n",
      "Step: 180 kappa: 0.5261074050711608\n",
      "Step: 230 kappa: 0.5193991577735966\n",
      "Step: 255 kappa: 0.5304376700201586\n",
      "Step: 302 kappa: 0.5228392238372865\n",
      "Step: 345 kappa: 0.5375826019264174\n",
      "Step: 410 kappa: 0.5281073120825818\n",
      "Step: 508 kappa: 0.5179475556473625\n",
      "Step: 527 kappa: 0.5149617730513935\n",
      "Step: 652 kappa: 0.5239221639048848\n",
      "Step: 672 kappa: 0.5162438341039733\n",
      "Step: 708 kappa: 0.5202537150741168\n",
      "Step: 796 kappa: 0.5287862408748393\n",
      "Step: 808 kappa: 0.525304079374853\n",
      "Step: 861 kappa: 0.5331271148734373\n",
      "Step: 898 kappa: 0.51642832361235\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kappa</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>class_0_value</th>\n",
       "      <th>class_1_value</th>\n",
       "      <th>class_2_value</th>\n",
       "      <th>class_3_value</th>\n",
       "      <th>class_4_value</th>\n",
       "      <th>class_5_value</th>\n",
       "      <th>num_cols (of 33)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.537583</td>\n",
       "      <td>350</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>418</td>\n",
       "      <td>418</td>\n",
       "      <td>369</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.537187</td>\n",
       "      <td>75</td>\n",
       "      <td>0.128889</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>654</td>\n",
       "      <td>418</td>\n",
       "      <td>421</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.535858</td>\n",
       "      <td>150</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>53</td>\n",
       "      <td>75</td>\n",
       "      <td>713</td>\n",
       "      <td>536</td>\n",
       "      <td>306</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0.533127</td>\n",
       "      <td>100</td>\n",
       "      <td>0.082222</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>595</td>\n",
       "      <td>300</td>\n",
       "      <td>432</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.530438</td>\n",
       "      <td>150</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>536</td>\n",
       "      <td>418</td>\n",
       "      <td>264</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>24</td>\n",
       "      <td>46</td>\n",
       "      <td>300</td>\n",
       "      <td>477</td>\n",
       "      <td>348</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>250</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>359</td>\n",
       "      <td>713</td>\n",
       "      <td>421</td>\n",
       "      <td>75</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>713</td>\n",
       "      <td>772</td>\n",
       "      <td>274</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "      <td>477</td>\n",
       "      <td>418</td>\n",
       "      <td>400</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>125</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.25</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>654</td>\n",
       "      <td>713</td>\n",
       "      <td>253</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        kappa  n_estimators  learning_rate  max_depth  subsample  \\\n",
       "345  0.537583           350       0.035556          2       0.25   \n",
       "68   0.537187            75       0.128889          2       0.50   \n",
       "116  0.535858           150       0.066667          2       0.75   \n",
       "861  0.533127           100       0.082222          3       0.75   \n",
       "255  0.530438           150       0.020000          2       0.25   \n",
       "..        ...           ...            ...        ...        ...   \n",
       "342  0.000000           100       0.160000          3       0.90   \n",
       "343  0.000000           250       0.035556          5       0.75   \n",
       "344  0.000000           300       0.051111          5       0.50   \n",
       "346  0.000000           100       0.144444          2       0.25   \n",
       "999  0.000000           125       0.144444          2       0.90   \n",
       "\n",
       "     colsample_bytree  class_0_value  class_1_value  class_2_value  \\\n",
       "345              0.25             10             75            418   \n",
       "68               0.75             32             53            654   \n",
       "116              0.50             53             75            713   \n",
       "861              0.25             17             32            595   \n",
       "255              0.50             61             10            536   \n",
       "..                ...            ...            ...            ...   \n",
       "342              0.50             24             46            300   \n",
       "343              0.25             46             61            359   \n",
       "344              0.25             39             32            713   \n",
       "346              0.90             46             39            477   \n",
       "999              0.25             24             68            654   \n",
       "\n",
       "     class_3_value  class_4_value  class_5_value  num_cols (of 33)  \n",
       "345            418            369             24                20  \n",
       "68             418            421             61                19  \n",
       "116            536            306             24                18  \n",
       "861            300            432             24                27  \n",
       "255            418            264             39                13  \n",
       "..             ...            ...            ...               ...  \n",
       "342            477            348             39                26  \n",
       "343            713            421             75                27  \n",
       "344            772            274             32                11  \n",
       "346            418            400             61                14  \n",
       "999            713            253             32                28  \n",
       "\n",
       "[1000 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A selection of parameters, these will be randomly selected from.\n",
    "\n",
    "n_estimators_values = [50, 75, 100, 125, 150, 200, 250, 300, 350]\n",
    "learning_rate_values = np.linspace(0.02, 0.16, 10)\n",
    "max_depth_values = [2,3,4,5]\n",
    "subsample_values = [0.25, 0.50, 0.75, 0.90]\n",
    "colsample_bytree_values = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "# Some target values appear a lot (max_value_count), some very few times (min_value_count) and some in the \n",
    "# middle (mid value count). We need these values for resampling.\n",
    "\n",
    "max_value_count = y_train.value_counts().max()\n",
    "mid_value_count = y_train.value_counts()[4]\n",
    "min_value_count = y_train.value_counts().min()\n",
    "\n",
    "# We select at random the number of target values in include\n",
    "\n",
    "class_values_small = np.linspace(10, 75, 10).round().astype('int')\n",
    "class_values_mid = np.linspace(mid_value_count-100, mid_value_count+100, 20).round().astype('int')\n",
    "class_values_large = np.linspace(300, max_value_count, 10).round().astype('int')\n",
    "\n",
    "# Fixed parameters\n",
    "\n",
    "cv_folds = 5\n",
    "tuning_iterations = 1000\n",
    "random.seed(2201020)\n",
    "skf_seed = random.randint(0, 2023)\n",
    "\n",
    "# K-fold split\n",
    "\n",
    "skf = StratifiedKFold(n_splits = cv_folds, random_state = skf_seed, shuffle = True)\n",
    "\n",
    "# Initialize for results storage\n",
    "\n",
    "tuning_results = defaultdict(list)\n",
    "valid_predictions = []\n",
    "eval_predictions = []\n",
    "feature_importances = []\n",
    "cols_selection = []\n",
    "completed_iterations = tuning_iterations\n",
    "\n",
    "# The loop\n",
    "\n",
    "for step in range(tuning_iterations):\n",
    "    # Choose random XGB parameters\n",
    "\n",
    "    n_estimators = random.choice(n_estimators_values)\n",
    "    learning_rate = random.choice(learning_rate_values)\n",
    "    max_depth = random.choice(max_depth_values)\n",
    "    subsample = random.choice(subsample_values)\n",
    "    colsample_bytree = random.choice(colsample_bytree_values)\n",
    "    \n",
    "    # Adding extra copies of data with particular target values, since these are under-represented\n",
    "    \n",
    "    class_0_value = random.choice(class_values_small)\n",
    "    class_1_value = random.choice(class_values_small)\n",
    "    class_2_value = random.choice(class_values_large)\n",
    "    class_3_value = random.choice(class_values_large)\n",
    "    class_4_value = random.choice(class_values_mid)\n",
    "    class_5_value = random.choice(class_values_small)\n",
    "    \n",
    "    X_train = X_train_origin\n",
    "    eval_data = eval_data_origin\n",
    "    \n",
    "    # Random column selection, here selecting at least one third\n",
    "    num_cols = random.randrange(X_train_origin.shape[1]//3, X_train_origin.shape[1])\n",
    "    rand_cols = random.sample(X_train_origin.columns.to_list(), num_cols)\n",
    "    cols_selection.append(rand_cols)\n",
    "    \n",
    "    X_train = X_train[rand_cols]\n",
    "    \n",
    "    # Initlialize for results storage\n",
    "    \n",
    "    kappas = []\n",
    "    test_probs = []\n",
    "    iter_valid_predictions = []\n",
    "    iter_eval_predictions = []\n",
    "    iter_feature_importance = np.zeros(num_cols)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_train_split, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_split, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # We need to join X and y before we resample\n",
    "        \n",
    "        merged = X_train_split.join(y_train_split)\n",
    "        \n",
    "        classes = []\n",
    "        for j in range(6):\n",
    "            this_class = merged[merged[target_col] == j]\n",
    "            classes.append(this_class)\n",
    "        \n",
    "        # Resample classes based on random selection above\n",
    "        \n",
    "        classes[0] = resample(classes[0], n_samples = class_0_value)\n",
    "        classes[1] = resample(classes[1], n_samples = class_1_value)\n",
    "        classes[2] = resample(classes[2], n_samples = class_2_value)\n",
    "        classes[3] = resample(classes[3], n_samples = class_3_value)\n",
    "        classes[4] = resample(classes[4], n_samples = class_4_value)\n",
    "        classes[5] = resample(classes[5], n_samples = class_5_value)\n",
    "        \n",
    "        # Rejoin classes\n",
    "        \n",
    "        merged = pd.concat(classes)\n",
    "        merged = merged.reset_index()\n",
    "        merged = merged.drop('index', axis = 1)\n",
    "        \n",
    "        # X, y split again\n",
    "        \n",
    "        X_train_split = merged.drop(target_col, axis=1)\n",
    "        y_train_split = merged[target_col]\n",
    "            \n",
    "        # XGB fit\n",
    "        \n",
    "        xgb_seed = random.randint(0, 2023)\n",
    "        xgb = XGBClassifier(n_estimators = n_estimators,\n",
    "                            learning_rate = learning_rate,\n",
    "                            max_depth = max_depth,\n",
    "                            subsample = subsample,\n",
    "                            colsample_bytree = colsample_bytree,\n",
    "                            random_state = xgb_seed).fit(X_train_split.values, y_train_split.values)\n",
    "        \n",
    "        # Keep track of feature importances for each XGB\n",
    "        \n",
    "        iter_feature_importance = iter_feature_importance + xgb.feature_importances_\n",
    "        \n",
    "        # XGB predict for valid data\n",
    "        \n",
    "        valid_prediction = xgb.predict(X_val.values)\n",
    "        valid_prediction = pd.DataFrame(valid_prediction, index=X_val.index, columns = [f'XGB_Step_{step}_Fold_{i}']) \n",
    "        iter_valid_predictions.append(valid_prediction)\n",
    "        \n",
    "        # Store kappa (the metric for this data) results\n",
    "        \n",
    "        kappa_result = cohen_kappa_score(y_val, valid_prediction, weights='quadratic')\n",
    "        kappas.append(kappa_result)\n",
    "        \n",
    "        # XGB predict for eval data\n",
    "        \n",
    "        eval_prediction = xgb.predict(eval_data[rand_cols].values)\n",
    "        eval_prediction = pd.DataFrame(eval_prediction, index=eval_data.index, columns = [f'XGB_Step_{step}_Fold_{i}'])\n",
    "        iter_eval_predictions.append(eval_prediction)\n",
    "        \n",
    "        # If we get a kappa of less than 0.495 on one the 5-fold slices then this doesn't look promissng and we \n",
    "        # skip to the next step in tuning iterations.\n",
    "        \n",
    "        if kappa_result < 0.495:\n",
    "            kappas = [0]\n",
    "            completed_iterations = completed_iterations - 1\n",
    "            break\n",
    "    \n",
    "    valid_predictions.append(pd.concat(iter_valid_predictions, axis=1))\n",
    "    eval_predictions.append(pd.concat(iter_eval_predictions, axis=1))\n",
    "    feature_importances.append((1/cv_folds) * iter_feature_importance)\n",
    "    \n",
    "    tuning_results['kappa'].append(np.mean(kappas))\n",
    "    tuning_results['n_estimators'].append(n_estimators)\n",
    "    tuning_results['learning_rate'].append(learning_rate)\n",
    "    tuning_results['max_depth'].append(max_depth)\n",
    "    tuning_results['subsample'].append(subsample)\n",
    "    tuning_results['colsample_bytree'].append(colsample_bytree)\n",
    "    tuning_results['class_0_value'].append(class_0_value)\n",
    "    tuning_results['class_1_value'].append(class_1_value)\n",
    "    tuning_results['class_2_value'].append(class_2_value)\n",
    "    tuning_results['class_3_value'].append(class_3_value)\n",
    "    tuning_results['class_4_value'].append(class_4_value)\n",
    "    tuning_results['class_5_value'].append(class_5_value)\n",
    "    tuning_results[f'num_cols (of {X_train_origin.shape[1]})'].append(len(rand_cols))\n",
    "    \n",
    "    if len(iter_valid_predictions) == cv_folds:\n",
    "        print(f'Step: {step} kappa: {np.mean(kappas)}')\n",
    "    \n",
    "tuning_results = pd.DataFrame(tuning_results)\n",
    "tuning_results.sort_values(by = 'kappa', axis = 0, inplace = True, ascending = False)\n",
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5b72d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kappa</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>class_0_value</th>\n",
       "      <th>class_1_value</th>\n",
       "      <th>class_2_value</th>\n",
       "      <th>class_3_value</th>\n",
       "      <th>class_4_value</th>\n",
       "      <th>class_5_value</th>\n",
       "      <th>num_cols (of 33)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.537583</td>\n",
       "      <td>350</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>418</td>\n",
       "      <td>418</td>\n",
       "      <td>369</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.537187</td>\n",
       "      <td>75</td>\n",
       "      <td>0.128889</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>654</td>\n",
       "      <td>418</td>\n",
       "      <td>421</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.535858</td>\n",
       "      <td>150</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>53</td>\n",
       "      <td>75</td>\n",
       "      <td>713</td>\n",
       "      <td>536</td>\n",
       "      <td>306</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0.533127</td>\n",
       "      <td>100</td>\n",
       "      <td>0.082222</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>595</td>\n",
       "      <td>300</td>\n",
       "      <td>432</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.530438</td>\n",
       "      <td>150</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>536</td>\n",
       "      <td>418</td>\n",
       "      <td>264</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0.528786</td>\n",
       "      <td>150</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>831</td>\n",
       "      <td>536</td>\n",
       "      <td>390</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.528489</td>\n",
       "      <td>250</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>75</td>\n",
       "      <td>32</td>\n",
       "      <td>831</td>\n",
       "      <td>536</td>\n",
       "      <td>432</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.528107</td>\n",
       "      <td>250</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>536</td>\n",
       "      <td>359</td>\n",
       "      <td>327</td>\n",
       "      <td>53</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.526107</td>\n",
       "      <td>50</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>477</td>\n",
       "      <td>418</td>\n",
       "      <td>358</td>\n",
       "      <td>68</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0.525304</td>\n",
       "      <td>200</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>654</td>\n",
       "      <td>300</td>\n",
       "      <td>379</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        kappa  n_estimators  learning_rate  max_depth  subsample  \\\n",
       "345  0.537583           350       0.035556          2       0.25   \n",
       "68   0.537187            75       0.128889          2       0.50   \n",
       "116  0.535858           150       0.066667          2       0.75   \n",
       "861  0.533127           100       0.082222          3       0.75   \n",
       "255  0.530438           150       0.020000          2       0.25   \n",
       "796  0.528786           150       0.020000          2       0.50   \n",
       "36   0.528489           250       0.051111          2       0.75   \n",
       "410  0.528107           250       0.020000          2       0.75   \n",
       "180  0.526107            50       0.051111          2       0.90   \n",
       "808  0.525304           200       0.051111          3       0.75   \n",
       "\n",
       "     colsample_bytree  class_0_value  class_1_value  class_2_value  \\\n",
       "345              0.25             10             75            418   \n",
       "68               0.75             32             53            654   \n",
       "116              0.50             53             75            713   \n",
       "861              0.25             17             32            595   \n",
       "255              0.50             61             10            536   \n",
       "796              0.25             24             75            831   \n",
       "36               0.75             75             32            831   \n",
       "410              0.50             32             53            536   \n",
       "180              0.75             53             39            477   \n",
       "808              0.50             17             53            654   \n",
       "\n",
       "     class_3_value  class_4_value  class_5_value  num_cols (of 33)  \n",
       "345            418            369             24                20  \n",
       "68             418            421             61                19  \n",
       "116            536            306             24                18  \n",
       "861            300            432             24                27  \n",
       "255            418            264             39                13  \n",
       "796            536            390             39                20  \n",
       "36             536            432             68                18  \n",
       "410            359            327             53                22  \n",
       "180            418            358             68                26  \n",
       "808            300            379             39                27  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351f3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we keep track of the feature importances for each XGB, not useful for the final model\n",
    "\n",
    "#feature_frames = []\n",
    "\n",
    "#for iter in range(50):\n",
    "#    feature_frame = pd.DataFrame([cols_selection[iter], feature_importances[iter]])\n",
    "#    feature_frame = feature_frame.transpose().sort_values(1, ascending=False).set_index(0)\n",
    "#    feature_frames.append(feature_frame)\n",
    "\n",
    "#pd.concat(feature_frames, axis=1).mean(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dfd31cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching over [0.         0.01282051 0.02564103 0.03846154 0.05128205 0.06410256\n",
      " 0.07692308 0.08974359 0.1025641  0.11538462 0.12820513 0.14102564\n",
      " 0.15384615 0.16666667 0.17948718 0.19230769 0.20512821 0.21794872\n",
      " 0.23076923 0.24358974 0.25641026 0.26923077 0.28205128 0.29487179\n",
      " 0.30769231 0.32051282 0.33333333 0.34615385 0.35897436 0.37179487\n",
      " 0.38461538 0.3974359  0.41025641 0.42307692 0.43589744 0.44871795\n",
      " 0.46153846 0.47435897 0.48717949 0.5       ]\n",
      "Checking 2\n",
      "Check 0\n",
      "[15. 14. 12. ... 15.  7. 15.]\n",
      "New best score 0.04628579687968559 with coefficients \n",
      " [0.2692307692307692, 0.47435897435897434]\n",
      "[8. 8. 7. ... 8. 4. 8.]\n",
      "New best score 0.1358214111434929 with coefficients \n",
      " [0.10256410256410256, 0.3076923076923077]\n",
      "[8. 7. 6. ... 8. 4. 8.]\n",
      "New best score 0.14070902511727867 with coefficients \n",
      " [0.14102564102564102, 0.24358974358974358]\n",
      "[6. 6. 5. ... 6. 3. 6.]\n",
      "New best score 0.2590801139525786 with coefficients \n",
      " [0.11538461538461538, 0.1923076923076923]\n",
      "[6. 5. 5. ... 6. 3. 6.]\n",
      "New best score 0.2851672373941766 with coefficients \n",
      " [0.1794871794871795, 0.11538461538461538]\n",
      "[3. 2. 2. ... 3. 1. 3.]\n",
      "New best score 0.30973856518728526 with coefficients \n",
      " [0.08974358974358974, 0.038461538461538464]\n",
      "[4. 4. 4. ... 4. 2. 4.]\n",
      "New best score 0.5437244608501439 with coefficients \n",
      " [0.10256410256410256, 0.10256410256410256]\n",
      "[4. 4. 3. ... 4. 2. 4.]\n",
      "New best score 0.5448608427098196 with coefficients \n",
      " [0.0641025641025641, 0.1282051282051282]\n",
      "Checking 3\n",
      "Check 0\n",
      "[4. 4. 4. ... 4. 2. 4.]\n",
      "New best score 0.5503900399069428 with coefficients \n",
      " [0.05128205128205128, 0.08974358974358974, 0.07692307692307693]\n",
      "[4. 4. 3. ... 4. 2. 4.]\n",
      "New best score 0.5525046260783579 with coefficients \n",
      " [0.02564102564102564, 0.0641025641025641, 0.1282051282051282]\n",
      "[4. 4. 3. ... 4. 2. 4.]\n",
      "New best score 0.5569905161487696 with coefficients \n",
      " [0.0641025641025641, 0.10256410256410256, 0.038461538461538464]\n",
      "Check 5000\n",
      "Check 10000\n",
      "Checking 4\n",
      "Check 0\n",
      "Check 5000\n",
      "[4. 4. 3. ... 4. 2. 4.]\n",
      "New best score 0.5647226226252191 with coefficients \n",
      " [0.038461538461538464, 0.05128205128205128, 0.07692307692307693, 0.038461538461538464]\n",
      "Check 10000\n",
      "Check 15000\n",
      "Checking 5\n",
      "Check 0\n",
      "Check 5000\n",
      "Check 10000\n",
      "Check 15000\n",
      "Check 20000\n",
      "Checking 6\n",
      "Check 0\n",
      "Check 5000\n",
      "Check 10000\n",
      "Check 15000\n",
      "Check 20000\n",
      "Check 25000\n",
      "Checking 7\n",
      "Check 0\n",
      "Check 5000\n",
      "Check 10000\n",
      "Check 15000\n",
      "Check 20000\n",
      "Check 25000\n",
      "Check 30000\n",
      "Checking 8\n",
      "Check 0\n",
      "Check 5000\n",
      "Check 10000\n",
      "Check 15000\n",
      "Check 20000\n",
      "Check 25000\n",
      "Check 30000\n",
      "Check 35000\n",
      "Checking 9\n",
      "Check 0\n",
      "Check 5000\n",
      "Check 10000\n",
      "Check 15000\n",
      "Check 20000\n",
      "Check 25000\n",
      "Check 30000\n",
      "Check 35000\n",
      "Check 40000\n"
     ]
    }
   ],
   "source": [
    "# We search for linear combinations of the best predictions. This is done by randomly selecting coefficients.\n",
    "# This seems like a clumsy way of proceeding, so we should look for a better one.\n",
    "\n",
    "# We check for combinations involving up to the 10 best predictions\n",
    "checking_range = 10\n",
    "\n",
    "# The best predictions. Here we need to sort_index, bizarely the Kaggle code didn't require this.\n",
    "best_preds = [cv_folds * valid_predictions[tuning_results.index[i]].sort_index().mean(axis=1) for i in range(checking_range)]\n",
    "\n",
    "# Initliatize and set parameters\n",
    "\n",
    "best_score = 0\n",
    "scaled_total = 0\n",
    "splits = 40\n",
    "linspc = np.linspace(0.0, 0.5, splits)\n",
    "\n",
    "print(f'Searching over {linspc}')\n",
    "\n",
    "# The loop\n",
    "\n",
    "for num_preds in range(2,checking_range):\n",
    "    print(f'Checking {num_preds}')\n",
    "    total_probe = min(num_preds * 5000, splits**num_preds)\n",
    "    for probe in range(total_probe):\n",
    "        if probe % 5000 == 0:\n",
    "            print(f'Check {probe}')\n",
    "        rnd = random.choices(linspc, k=num_preds)\n",
    "        scaled_total = np.matmul(np.array(rnd), np.array(best_preds[0:num_preds]))\n",
    "        scaled_total = scaled_total.round()\n",
    "        score = cohen_kappa_score(y_train, scaled_total, weights='quadratic')\n",
    "        if score > best_score:\n",
    "            print(scaled_total)\n",
    "            best_score = score\n",
    "            best_coefficients = rnd\n",
    "            best_num_preds = num_preds\n",
    "            print(f'New best score {score} with coefficients \\n {rnd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60abda",
   "metadata": {},
   "source": [
    "# Results and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "743cb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [eval_predictions[tuning_results.index[i]].fillna(0).mean(axis=1) for i in range(best_num_preds)]\n",
    "\n",
    "result = np.array(result)\n",
    "\n",
    "result = pd.DataFrame(np.matmul(best_coefficients, result).round()).astype('int64')\n",
    "\n",
    "submission = sample_submit.copy()\n",
    "\n",
    "submission[target_col] = result\n",
    "submission[target_col] = le.inverse_transform(submission[target_col])\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
